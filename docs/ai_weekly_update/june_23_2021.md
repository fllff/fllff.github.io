---
layout: default
title: 2021 June 23rd
nav_order: -4
parent: AI Weekly Updates
---

## Multi-head or Single-head? An Empirical Comparison for Transformer Training

### 개요
- [https://arxiv.org/pdf/2106.09650.pdf](https://arxiv.org/pdf/2106.09650.pdf)
- 다양한 position을 jointly attend 하는것은 multi-head attention의 unique한 특징은 아니다
    - multi-layer single-head attention으로도 효과적인 attention이 가능함 보임
- 같은 size의 attention에서 multi-head attention이 train stability가 좋았다.
    - 24-layer 16-head VS 384-layer single-head    
    - deep single-head transformer is harder to train

=> deep single-head transformer에서 Admin이라는 initialization 방법을 사용해서 학습 난이도를 낮추고, 결국 더 좋은 성능을 얻었다고 하는데 정확한 비교인지는 잘 모르겠다.

--------------------

## Prompting Contrastive Explanations for Commonsense Reasoning Tasks

### 개요
- [https://arxiv.org/pdf/2106.06823.pdf](https://arxiv.org/pdf/2106.06823.pdf)
- PLM은 commonsense reasoning에 강하지만, 왜 그렇게 추리했는지 알아내기 힘듦
- 추리의 evidence를 생성하게 하는 방법 제안
    - PLM에게 두 대조되는 키워드를 비교하는 문장을 완성하도록 함

### 내용
- 템플릿 
    - ![image](https://user-images.githubusercontent.com/6601619/124637515-cc735980-dec4-11eb-8796-dbf1dfea452b.png)
    - annotator들로부터 예시를 만들게 한 후, 그것을 통해 template들을 정의.
    - template의 _ 를 모델이 채우도록 할 예정

- explanation 생성
    - ![image](https://user-images.githubusercontent.com/6601619/124636363-6c2fe800-dec3-11eb-9f78-09341f5468f8.png)
    - template들에 대하여, explanation 후보들을 생성
    - 별도의 fine-tune없이 PLM 그대로 사용

- explanation 선택모델 만들기
    - 두 대조에 있는 키워드 a1, a2에 대하여, 
    - 각 키워드를 답으로 하는 explanation 후보들을, PLM으로 loss 측정후 값을 다 더하는 layer 정의
    - 답에 해당하는 키워드의 loss가 더 작도록 PLM fine-tuning
